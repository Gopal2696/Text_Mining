{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n"
     ]
    }
   ],
   "source": [
    "# pip install xlrd\n",
    "# import xlrd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk \n",
    "import re, string, unicodedata\n",
    "import sys\n",
    "import urllib\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "import urllib2  \n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# import xlrd\n",
    "\n",
    "def syllables(words):\n",
    "    sum1=0\n",
    "    for word in words:\n",
    "        count = 0\n",
    "#         vowels = 'aeiouy'\n",
    "        if word.endswith(('es','ed')):\n",
    "                for w in word:\n",
    "                     if(w=='a' or w=='e' or w=='i' or w=='o' or w=='u'):\n",
    "                        count += 1\n",
    "                if(count>3):\n",
    "                     sum1+=sum1\n",
    "        else:\n",
    "            for w in word:\n",
    "                if(w=='a' or w=='e' or w=='i' or w=='o' or w=='u'):\n",
    "                        count += 1\n",
    "            if(count>2):\n",
    "                 sum1+=sum1\n",
    "    return sum1\n",
    "\n",
    "\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    stopset = set(w.upper() for w in stopwords.words('english'))\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopset:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "\n",
    "def normalize_making(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.encode('ascii', 'ignore').decode('ascii').strip()\n",
    "\n",
    "        new_word = re.sub('\\[[^]]*\\]', '', new_word)\n",
    "        new_word = re.sub('[\\d%/$]', '', new_word)\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', new_word)\n",
    "        if new_word != '':\n",
    "            new_words.append(word.upper())\n",
    "    return new_words\n",
    "\n",
    "  \n",
    "\n",
    "    \n",
    "positive_negative = pd.read_csv('LoughranMcDonald_MasterDictionary_2016.csv', index_col= 0)\n",
    "constraining = set(pd.read_excel('constraining_dictionary.xlsx',index_col = 0).index)\n",
    "uncertainty = set(pd.read_excel('uncertainty_dictionary.xlsx', index_col = 0).index)\n",
    "x1  = pd.ExcelFile('cik_list.xlsx')\n",
    "df1 = x1.parse('cik_list_ajay')\n",
    "section_name_excel = ['MDA','QQDMR','RF']\n",
    "variables = ['positive_score','negative_score','polarity_score','average_sentence_length', 'percentage_of_complex_words',\\\n",
    "                   'fog_index','complex_word_count','word_count','uncertainty_score','constraining_score', 'positive_word_proportion',\\\n",
    "                   'negative_word_proportion', 'uncertainty_word_proportion', 'constraining_word_proportion']\n",
    "df_col = []\n",
    "for i in section_name_excel:\n",
    "    for j in variables:\n",
    "            k = i.lower() + '_' + j\n",
    "            df_col.append(k)\n",
    "    df2 = pd.DataFrame(columns=df_col)\n",
    "    \n",
    "    \n",
    "    \n",
    "max_row, max_col = df1.shape\n",
    "df1['SECFNAME'] = df1['SECFNAME'].apply(lambda x: \"{}{}\".format('https://www.sec.gov/Archives/', x))\n",
    "constraining_whole = pd.Series(name='constraining_whole')\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "def main_function(str1):\n",
    "    sentence = sent_tokenize(str1)\n",
    "    sentence_length = len(sentence)\n",
    "    \n",
    "    sent_list = word_tokenize(str1)\n",
    "    word_count = len(sent_list)\n",
    "    uncertainty_score = 0 \n",
    "    constraining_score= 0\n",
    "    positive_score = 0\n",
    "    negative_score = 0\n",
    "    fog_index = 0\n",
    "    percentage_of_complex_words = 0\n",
    "    polarity_score = 0\n",
    "    positive_word_proportion = 0 \n",
    "    negative_word_proportion =  0 \n",
    "    uncertainty_word_proportion = 0 \n",
    "    constraining_word_proportion = 0 \n",
    "    average_sentence_length=0\n",
    "    complex_words=0\n",
    "    \n",
    "    if word_count!= 0 :\n",
    "        average_sentence_length = word_count/sentence_length\n",
    "\n",
    "        \n",
    "        sent_list = remove_stopwords(sent_list)\n",
    "        complex_words = syllables(sent_list)\n",
    "        sent_list = normalize_making(sent_list)\n",
    "        percentage_of_complex_words = complex_words/word_count\n",
    "        fog_index = 0.4 * (average_sentence_length + percentage_of_complex_words)\n",
    "        \n",
    "\n",
    "        new_words = []\n",
    "        sent_list = normalize_making(sent_list)\n",
    "\n",
    "        for word in sent_list:\n",
    "            if word in positive_negative.index:\n",
    "                if positive_negative.loc[word].Positive > 0:\n",
    "                        positive_score += 1\n",
    "                if positive_negative.loc[word].Negative > 0:\n",
    "                        negative_score += 1\n",
    "            if word in uncertainty:\n",
    "                        uncertainty_score += 1\n",
    "            if word in constraining:\n",
    "                        constraining_score += 1\n",
    "        \n",
    "        polarity_score = (positive_score-negative_score)/(positive_score + negative_score + .000001)\n",
    "        positive_word_proportion = positive_score/word_count\n",
    "        negative_word_proportion = negative_score/word_count\n",
    "        uncertainty_word_proportion = uncertainty_score/word_count\n",
    "        constraining_word_proportion = constraining_score/word_count\n",
    "    variable_names = [positive_score,negative_score,polarity_score,average_sentence_length,percentage_of_complex_words,\\\n",
    "                     fog_index, complex_words, word_count, uncertainty_score,constraining_score, positive_word_proportion,\\\n",
    "                     negative_word_proportion, uncertainty_word_proportion, constraining_word_proportion]\n",
    "    return variable_names\n",
    "\n",
    "\n",
    "\n",
    "###################################################################################\n",
    "    \n",
    "for row in range(max_row):\n",
    "    i = df1.loc[row]['SECFNAME']\n",
    "    r = requests.get(i)    \n",
    "    soup = BeautifulSoup(r.text, 'html.parser',from_encoding=\"utf-8\")\n",
    "    stripped_text = soup.get_text()\n",
    "\n",
    "\n",
    "    with open('myfile1.txt', 'w+') as f:\n",
    "        print(row)\n",
    "        the_text = (stripped_text).encode('utf-8')\n",
    "        file = f.write(the_text)\n",
    "    \n",
    "    \n",
    "############################################################################################    \n",
    "    \n",
    "    recording = False\n",
    "    output_section_mda = []\n",
    "    for line in open('myfile1.txt').readlines():\n",
    "\n",
    "        if recording is False:\n",
    "            if re.search(r'ITEM [0-9]. MANAGEMENT\\'S DISCUSSION AND ANALYSIS', line) is not None:\n",
    "                recording = True\n",
    "                output_section_mda.append(line.decode('utf-8').strip())\n",
    "        elif recording is True:\n",
    "            if re.search(r'ITEM [0-9].', line) is not None:\n",
    "                recording = False\n",
    "            output_section_mda.append(line.decode('utf-8').strip())\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    str1 = ''.join(output_section_mda)\n",
    "    variable_names = main_function(str1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    df2.loc[row] = np.zeros(42)\n",
    "\n",
    "    k=\"MDA\"\n",
    "    length=0\n",
    "    for j in variables:\n",
    "        col = k.lower() + '_' + j\n",
    "        df2.loc[row][col] = variable_names[length]\n",
    "        length+=1\n",
    "\n",
    "        \n",
    "#########################################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    recording = False\n",
    "    output_section_QQDMR = []\n",
    "    for line in open('myfile1.txt').readlines():\n",
    "\n",
    "        if recording is False:\n",
    "            if re.search(r'ITEM [0-9]. Quantitative and Qualitative Disclosures about Market Risk', line) is not None:\n",
    "                recording = True\n",
    "                output_section_QQDMR.append(line.decode('utf-8').strip())\n",
    "        elif recording is True:\n",
    "            if re.search(r'ITEM [0-9].', line) is not None:\n",
    "                recording = False\n",
    "            output_section_QQDMR.append(line.decode('utf-8').strip())\n",
    "    str1 = ''.join(output_section_QQDMR)\n",
    "\n",
    "    \n",
    "    variable_names = main_function(str1)\n",
    "\n",
    "    k=\"QQDMR\"\n",
    "    length=0\n",
    "    for j in variables:\n",
    "        col = k.lower() + '_' + j\n",
    "        df2.loc[row][col] = variable_names[length]\n",
    "        length+=1\n",
    "    \n",
    "    \n",
    "#########################################################################################    \n",
    "    \n",
    "    recording = False\n",
    "    output_section_rf = []\n",
    "    for line in open('myfile1.txt').readlines():\n",
    "\n",
    "        if recording is False:\n",
    "            if re.search(r'ITEM [0-9]. Risk Factors', line) is not None:\n",
    "                recording = True\n",
    "                output_section_rf.append(line.decode('utf-8').strip())\n",
    "        elif recording is True:\n",
    "            if re.search(r'ITEM [0-9].', line) is not None:\n",
    "                recording = False\n",
    "            output_section_rf.append(line.decode('utf-8').strip())\n",
    "    str1 = ''.join(output_section_rf)\n",
    "\n",
    "    \n",
    "    variable_names = main_function(str1)\n",
    "\n",
    "\n",
    "    k=\"rf\"\n",
    "    length=0\n",
    "    for j in variables:\n",
    "        col = k.lower() + '_' + j\n",
    "        df2.loc[row][col] = variable_names[length]\n",
    "        length+=1\n",
    "        \n",
    "##############################################################################################        \n",
    "    output_section_whole = [] \n",
    "    for line in open('myfile1.txt').readlines():\n",
    "        output_section_whole.append(line.decode('utf-8').strip())\n",
    "    str1 = ''.join(output_section_whole)\n",
    "    sent_list = word_tokenize(str1)\n",
    "    sent_list = remove_stopwords(sent_list)\n",
    "    sent_list = normalize_making(sent_list)\n",
    "    \n",
    "    constraining_whole.loc[row] = 0\n",
    "    constraining_words_whole_report_count = 0\n",
    "    for word in sent_list:\n",
    "        \n",
    "        if word in constraining:\n",
    "            constraining_words_whole_report_count += 1\n",
    "   \n",
    "    constraining_whole.loc[row] = constraining_words_whole_report_count\n",
    "    \n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "df = pd.concat([df1,df2,constraining_whole], axis = 1)\n",
    "# df.shape    \n",
    "    \n",
    "# print(df)   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('output.xlsx')\n",
    "df.to_excel(writer,'Sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
